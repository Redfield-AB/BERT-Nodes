<?xml version="1.0" encoding="UTF-8"?>
<knimeNode icon="./bert_selector.png" type="Source" xmlns="http://knime.org/node/v2.10" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://knime.org/node/v2.10 http://knime.org/node/v2.10.xsd">
    <name>BERT Model Selector</name>
    
    <shortDescription>
        The node allows downloading the model available on TensorFlow Hub and HuggingFace.
    </shortDescription>
    
    <fullDescription>
        <intro>
        	The node allows downloading the model available on TensorFlow Hub and HuggingFace.
        	The trusted models are added to the lists.
        	For HuggingFace it is possible to paste the model name into the selector.
        	In case you would like to test other models please refer to Advanced tab.
        	The model can be cached on a disk.
        </intro>
        <tab name="Settings">
        	<option name="Select Model">Model selection from the TensorFlow Hub list or Huggingface list.</option>
        	<option name="Cache directory">A path where the models will be stored for further usage.</option>
        	<option name="Remote URL">
				An arbitrary link to a model. Use with caution since there is no guarantee that the model will be compatible with the node.
				<b>Only active when Advanced tab checkbox is active. This feature is experimental, use with discretion.</b>
			</option>
        	<option name="Local folder">
        		A path to the local model that was downloaded not by this node.
        		<b>Only active when Advanced tab checkbox is active. This feature is experimental, use with discretion.</b>
        	</option>
        </tab>
        <tab name="Advanced">
        	<option name="Enable Remote URL and Local path selection modes">
        		Enable Remote URL and Local path selection modes - an experimental feature that makes it possible to pick any model from a remote or local source.
        		There is no guarantee that the models will be compatible with the BERT extension.
        	</option>
        </tab>
        <tab name="Python">
    		<option name="Python">
    			Select one of Python execution environment options:
    			<ul>
        			<li>use default Python environment for Deep Learning</li>
        			<li>use Conda environment</li>
        		</ul>
    		</option>
        </tab>        
    </fullDescription>
    
    <ports>
		<outPort name="BERT Model" index="0">The BERT model that was downloaded or read from cache.</outPort>
    </ports>    
</knimeNode>
